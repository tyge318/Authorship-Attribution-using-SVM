{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import grid_search\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDocument(file):\n",
    "    lines = file.split(\"\\n\")\n",
    "    n = len(lines)\n",
    "    begin, end = n//3, 2*n//3\n",
    "    lines = lines[begin:end]\n",
    "    lines = (line for line in lines if len(line)>0)\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_dict = {}\n",
    "def loadData(path):\n",
    "    documents, authors = [], []\n",
    "    subfolders = [subfolder for subfolder in os.listdir(path) if os.path.isdir(os.path.join(path, subfolder))]\n",
    "    for i, subfolder in enumerate(subfolders):\n",
    "        subpath = os.path.join(path, subfolder)\n",
    "        authors_dict[i] = subfolder\n",
    "        for f_name in os.listdir(subpath):\n",
    "            if not f_name.endswith('.txt'):\n",
    "                continue\n",
    "            with open(os.path.join(subpath, f_name), 'r', encoding='utf-8') as f:\n",
    "                cleanedText = cleanDocument(f.read())\n",
    "                documents.append(cleanedText)\n",
    "                authors.append(i)\n",
    "    return documents, np.array(authors, dtype='int')\n",
    "documents_all, authors_all = loadData('datasets')\n",
    "documents, documents_test, authors, authors_test = train_test_split(documents_all, authors_all, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_words = [\"a\", \"able\", \"aboard\", \"about\", \"above\", \"absent\",\"according\" , \"accordingly\", \"across\", \"after\", \"against\",\n",
    "                  \"ahead\", \"albeit\", \"all\", \"along\", \"alongside\", \"although\", \"am\", \"amid\", \"amidst\", \"among\", \"amongst\", \"amount\", \"an\",\n",
    "                  \"and\", \"another\", \"anti\", \"any\", \"anybody\", \"anyone\", \"anything\", \"are\", \"around\", \"as\", \"aside\", \"astraddle\",\n",
    "                  \"astride\", \"at\", \"away\", \"bar\", \"barring\", \"be\", \"because\", \"been\", \"before\", \"behind\", \"being\", \"below\", \"beneath\",\n",
    "                  \"beside\", \"besides\", \"better\", \"between\", \"beyond\", \"bit\", \"both\", \"but\", \"by\", \"can\", \"certain\", \"circa\", \"close\",\n",
    "                  \"concerning\", \"consequently\", \"considering\", \"could\", \"couple\", \"dare\", \"deal\", \"despite\", \"down\", \"due\", \"during\",\n",
    "                  \"each\", \"eight\", \"eighth\", \"either\", \"enough\", \"every\", \"everybody\", \"everyone\", \"everything\", \"except\", \"excepting\",\n",
    "                  \"excluding\", \"failing\", \"few\", \"fewer\", \"fifth\", \"first\", \"five\", \"following\", \"for\", \"four\", \"fourth\", \"from\", \"front\",\n",
    "                  \"given\", \"good\", \"great\", \"had\", \"half\", \"have\", \"he\", \"heaps\", \"hence\", \"her\", \"hers\", \"herself\", \"him\", \"himself\",\n",
    "                  \"his\", \"however\", \"i\", \"if\", \"in\", \"including\", \"inside\", \"instead\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keeping\",\n",
    "                  \"lack\", \"less\", \"like\", \"little\", \"loads\", \"lots\", \"majority\", \"many\", \"masses\", \"may\", \"me\", \"might\", \"mine\", \"minority\",\n",
    "                  \"minus\", \"more\", \"most\", \"much\", \"must\", \"my\", \"myself\", \"near\", \"need\", \"neither\", \"nevertheless\", \"next\", \"nine\",\n",
    "                  \"ninth\", \"no\", \"nobody\", \"none\", \"nor\", \"nothing\", \"notwithstanding\", \"number\", \"numbers\", \"of\", \"off\", \"on\",\n",
    "                  \"once\", \"one\", \"onto\", \"opposite\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"part\",\n",
    "                  \"past\", \"pending\", \"per\", \"pertaining\", \"place\", \"plenty\", \"plethora\", \"plus\", \"quantities\", \"quantity\", \"quarter\",\n",
    "                  \"regarding\", \"remainder\", \"respecting\", \"rest\", \"round\", \"save\", \"saving\", \"second\", \"seven\", \"seventh\", \"several\",\n",
    "                  \"shall\", \"she\", \"should\", \"similar\", \"since\", \"six\", \"sixth\",\"so\", \"some\", \"somebody\", \"someone\", \"something\", \"spite\",\n",
    "                  \"such\", \"ten\", \"tenth\", \"than\", \"thanks\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\",\n",
    "                  \"therefore\", \"these\", \"they\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\",\n",
    "                  \"till\", \"time\", \"to\", \"tons\", \"top\", \"toward\", \"towards\", \"two\", \"under\", \"underneath\", \"unless\", \"unlike\", \"until\",\n",
    "                  \"unto\", \"up\", \"upon\", \"us\", \"used\", \"various\", \"versus\", \"via\", \"view\", \"wanting\", \"was\", \"we\", \"were\", \"what\",\n",
    "                  \"whatever\", \"when\", \"whenever\", \"where\", \"whereas\", \"wherever\", \"whether\", \"which\", \"whichever\", \"while\",\n",
    "                  \"whilst\", \"who\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whose\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\",\n",
    "                  \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification with function words:\n",
      "Best parameters: {'C': 1, 'kernel': 'linear'}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         1\n",
      "          1       1.00      1.00      1.00         2\n",
      "          2       1.00      0.80      0.89         5\n",
      "          3       0.60      1.00      0.75         3\n",
      "          4       1.00      0.75      0.86         4\n",
      "\n",
      "avg / total       0.92      0.87      0.87        15\n",
      "\n",
      "Classification with unigram and bigram:\n",
      "Best parameters: {'C': 1, 'kernel': 'linear'}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         1\n",
      "          1       0.67      1.00      0.80         2\n",
      "          2       1.00      0.80      0.89         5\n",
      "          3       0.50      0.67      0.57         3\n",
      "          4       1.00      0.75      0.86         4\n",
      "\n",
      "avg / total       0.86      0.80      0.81        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def svmWithExtractor(extractor):\n",
    "    X = extractor.fit_transform(documents)\n",
    "    vocs = extractor.vocabulary_\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, authors, test_size=0.2, random_state=1)\n",
    "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "    svm = SVC()\n",
    "    grid = grid_search.GridSearchCV(svm, parameters)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print('Best parameters:', grid.best_params_)\n",
    "    y_true, y_pred = y_val, grid.predict(X_val)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return grid, vocs\n",
    "print('Classification with function words:')\n",
    "svmFW, _ = svmWithExtractor(CountVectorizer(vocabulary=function_words))\n",
    "print('Classification with unigram and bigram:')\n",
    "svmNG, NG_vocs = svmWithExtractor(CountVectorizer(analyzer='word', ngram_range=(1, 2)))\n",
    "print('Classification with unigram tfidf:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with function words:\n",
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is MarkTwain, predict author is MarkTwain\n",
      "True author is MarkTwain, predict author is MarkTwain\n",
      "True author is Shakespeare, predict author is Shakespeare\n",
      "True author is CharlesDickens, predict author is MarkTwain\n",
      "True author is BoothTarkington, predict author is BoothTarkington\n",
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is CharlesDickens, predict author is JulesVerne\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is CharlesDickens, predict author is BoothTarkington\n",
      "True author is BoothTarkington, predict author is BoothTarkington\n",
      "True author is MarkTwain, predict author is MarkTwain\n",
      "\n",
      "Prediction with unigram and bigram:\n",
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is MarkTwain, predict author is MarkTwain\n",
      "True author is MarkTwain, predict author is MarkTwain\n",
      "True author is Shakespeare, predict author is Shakespeare\n",
      "True author is CharlesDickens, predict author is MarkTwain\n",
      "True author is BoothTarkington, predict author is BoothTarkington\n",
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is CharlesDickens, predict author is BoothTarkington\n",
      "True author is BoothTarkington, predict author is BoothTarkington\n",
      "True author is MarkTwain, predict author is MarkTwain\n"
     ]
    }
   ],
   "source": [
    "def predictWithExtractor(svm, extractor):\n",
    "    X = extractor.fit_transform(documents_test)\n",
    "    y_preds = svm.predict(X)\n",
    "    for i, y_pred in enumerate(y_preds):\n",
    "        y_true = authors_test[i]\n",
    "        print('True author is %s, predict author is %s' % (authors_dict[y_true], authors_dict[y_pred]))\n",
    "        \n",
    "print('Prediction with function words:')\n",
    "predictWithExtractor(svmFW, CountVectorizer(vocabulary=function_words))\n",
    "print('\\nPrediction with unigram and bigram:')\n",
    "predictWithExtractor(svmNG, CountVectorizer(analyzer='word', ngram_range=(1, 2), vocabulary = NG_vocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(arr, n):\n",
    "    ret = [0]*n\n",
    "    for i in arr:\n",
    "        if i >= n:\n",
    "            ret[n-1] += 1\n",
    "        elif i > 0:\n",
    "            ret[i-1] += 1\n",
    "    return ret\n",
    "def sentenceLengths(tokenized_sentences):  \n",
    "    t = map(len, tokenized_sentences)\n",
    "    return helper(t, 26)\n",
    "def wordLengths(words):\n",
    "    t = map(len, words)\n",
    "    return helper(t, 18)\n",
    "def pronounPerSentence(tokenized_sentences):\n",
    "    pronouns = set(['something', 'thou', 'everybody', 'all', 'anything', 'your', 'ours', 'her', 'myself', 'him', \n",
    "                    'everything', 'I', 'nobody', 'somebody', 'whomever', 'who', 'everyone', 'none', 'each', 'thee', \n",
    "                    'thy', 'anybody', 'nothing', 'this', 'one', 'our', 'his', 'we', 'yourself', 'they', 'another', \n",
    "                    'himself', 'me', 'several', 'hers', 'no one', 'ourselves', 'both', 'some', 'itself', 'my', \n",
    "                    'whose', 'these', 'other', 'either', 'someone', 'few', 'themselves', 'thine', 'whichever', \n",
    "                    'neither', 'as', 'he', 'she', 'theirs', 'which', 'such', 'mine', 'whom', 'that', 'yourselves', \n",
    "                    'whoever', 'what', 'those', 'others', 'whatever', 'it', 'them', 'us', 'anyone', 'their', 'most', \n",
    "                    'yours', 'any', 'many', 'herself', 'you'])\n",
    "    def countPronouns(sentence):\n",
    "        cnt = 0\n",
    "        for word in sentence:\n",
    "            if word in pronouns:\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "    pronounCnts = map(countPronouns, tokenized_sentences)\n",
    "    return helper(pronounCnts, 10)\n",
    "def conujnctionPerSentence(tokenized_sentences):\n",
    "    conjunctions = set(['and','that','but','or','as','if','when','than','because','while','where',\n",
    "                        'after','so','though','since','until','whether','before','although','nor','like',\n",
    "                        'once','unless','now','except'])\n",
    "    def countConjunctions(sentence):\n",
    "        cnt = 0\n",
    "        for word in sentence:\n",
    "            if word in conjunctions:\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "    conjunctionCnts = map(countConjunctions, tokenized_sentences)\n",
    "    return helper(conjunctionCnts, 7)\n",
    "def customFeatureExtractor(document):\n",
    "    features = []\n",
    "    # number of unique words\n",
    "    word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = word_tokenizer.tokenize(document.lower())\n",
    "    features.append(len(set(words)))\n",
    "    # sentence lengths\n",
    "    sentences = sent_tokenize(document.lower())\n",
    "    tokenized_sentences = list(map(word_tokenizer.tokenize, sentences))\n",
    "    sl = sentenceLengths(tokenized_sentences)\n",
    "    features.extend(sl)\n",
    "    # word lengths\n",
    "    wl = wordLengths(words)\n",
    "    features.extend(wl)\n",
    "    # pronoun per sentences\n",
    "    pps = pronounPerSentence(tokenized_sentences)\n",
    "    features.extend(pps)\n",
    "    # conjunction per sentences\n",
    "    cps = conujnctionPerSentence(tokenized_sentences)\n",
    "    features.extend(cps)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'kernel': 'linear'}\n",
      "[2 2 1 4 3 1 3 4 3 3 3 2 2 0 4]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         1\n",
      "          1       0.50      0.50      0.50         2\n",
      "          2       1.00      0.80      0.89         5\n",
      "          3       0.60      1.00      0.75         3\n",
      "          4       1.00      0.75      0.86         4\n",
      "\n",
      "avg / total       0.85      0.80      0.81        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def svmCustom():\n",
    "    X = np.asarray(list(map(customFeatureExtractor, documents)))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, authors, test_size=0.2, random_state=1)\n",
    "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "    svm = SVC()\n",
    "    grid = grid_search.GridSearchCV(svm, parameters)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print('Best parameters:', grid.best_params_)\n",
    "    y_true, y_pred = y_val, grid.predict(X_val)\n",
    "    print(y_pred)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return grid\n",
    "svmCF = svmCustom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is JulesVerne, predict author is MarkTwain\n",
      "True author is MarkTwain, predict author is CharlesDickens\n",
      "True author is MarkTwain, predict author is MarkTwain\n",
      "True author is Shakespeare, predict author is Shakespeare\n",
      "True author is CharlesDickens, predict author is BoothTarkington\n",
      "True author is BoothTarkington, predict author is BoothTarkington\n",
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is JulesVerne, predict author is MarkTwain\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is JulesVerne, predict author is JulesVerne\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is CharlesDickens, predict author is JulesVerne\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is CharlesDickens, predict author is CharlesDickens\n",
      "True author is BoothTarkington, predict author is BoothTarkington\n",
      "True author is MarkTwain, predict author is CharlesDickens\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(list(map(customFeatureExtractor, documents_test)))\n",
    "y_preds = svmCF.predict(X)\n",
    "for i, y_pred in enumerate(y_preds):\n",
    "    y_true = authors_test[i]\n",
    "    print('True author is %s, predict author is %s' % (authors_dict[y_true], authors_dict[y_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
