{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function to extract text\n",
    "The Gutenberg project adds extra information to the begin and end of each ebook, we don't want these text to affect the result. Also, there's no need to include complete book content; all we need is enough text to know the style of the author. Therefore, here we partition the book into 3 parts and take only the middle part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanDocument(file):\n",
    "    lines = file.split(\"\\n\")\n",
    "    n = len(lines)\n",
    "    begin, end = n//3, 2*n//3\n",
    "    lines = lines[begin:end]\n",
    "    lines = (line for line in lines if len(line)>0)\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "The data is kept in the directory \"dataset\" and categorized into subfolders with respect to each author. Thus, the label for each document will be the subfolder (in integer value) where it locates. A look-up table is generated so that we can map the integer values to subfolder name in string. Finally, do a train-test split and keep 0.2 of the data as our testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "authors_dict = {}\n",
    "def loadData(path):\n",
    "    documents, authors = [], []\n",
    "    subfolders = [subfolder for subfolder in os.listdir(path) if os.path.isdir(os.path.join(path, subfolder))]\n",
    "    for i, subfolder in enumerate(subfolders):\n",
    "        subpath = os.path.join(path, subfolder)\n",
    "        authors_dict[i] = subfolder\n",
    "        for f_name in os.listdir(subpath):\n",
    "            if not f_name.endswith('.txt'):\n",
    "                continue\n",
    "            with open(os.path.join(subpath, f_name), 'r', encoding='utf-8') as f:\n",
    "                cleanedText = cleanDocument(f.read())\n",
    "                documents.append(cleanedText)\n",
    "                authors.append(i)\n",
    "    return documents, np.array(authors, dtype='int')\n",
    "documents_all, authors_all = loadData('datasets')\n",
    "documents, documents_test, authors, authors_test = train_test_split(documents_all, authors_all, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function to lead function word from txt file\n",
    "The usage of function words is more dependent on the author's decision, so they are good features to decide an author's writing style. The function word list is obtained from https://semanticsimilarity.wordpress.com/function-word-lists/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_function_words(path):\n",
    "    function_words = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            function_words.append(line.split()[0])\n",
    "    return function_words\n",
    "function_words = load_function_words('function_words.txt')\n",
    "#print(authors_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The models and training\n",
    "Here I use three different models: function word features, n-gram features (unigram and bigram) and TfIdf features. These features can be easily extracted with scikit-learn's vectorizers. I use grid search function to determine the best parameter settings, and I output the trained model the fitted vectorizer for later use in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification with function words:\n",
      "Best parameters: {'C': 1, 'kernel': 'linear'}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         1\n",
      "          1       0.67      1.00      0.80         2\n",
      "          2       1.00      1.00      1.00         4\n",
      "          3       1.00      0.67      0.80         3\n",
      "          4       1.00      1.00      1.00         5\n",
      "\n",
      "avg / total       0.96      0.93      0.93        15\n",
      "\n",
      "Classification with unigram and bigram:\n"
     ]
    }
   ],
   "source": [
    "def svmWithExtractor(extractor):\n",
    "    X = extractor.fit_transform(documents)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, authors, test_size=0.2, random_state=2)\n",
    "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "    svm = SVC()\n",
    "    grid = GridSearchCV(svm, parameters)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print('Best parameters:', grid.best_params_)\n",
    "    y_true, y_pred = y_val, grid.predict(X_val)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(report)\n",
    "    values = np.array(list(map(float, report.split('\\n')[-2].split()[3:6])))\n",
    "    return grid, extractor\n",
    "print('Classification with function words:')\n",
    "svmFW, FWextractor = svmWithExtractor(CountVectorizer(vocabulary=function_words))\n",
    "print('Classification with unigram and bigram:')\n",
    "svmNG, NGextractor = svmWithExtractor(CountVectorizer(analyzer='word', ngram_range=(1, 2)))\n",
    "print('Classification with unigram TfIdf:')\n",
    "svmTfIdf, TfIdfextractor = svmWithExtractor(TfidfVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run prediction\n",
    "Now we're ready to run prediction with the trained model and test set. We need to use the fitted vectorizer from training stage to do feature extraction. I loop through each testing document and compare the true label with predicted label. The averaged performance metric (precision, recall and f1-score) is returned so that we can plot to compare the performance of different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictWithExtractor(svm, extractor):\n",
    "    X = extractor.transform(documents_test)\n",
    "    y_preds = svm.predict(X)\n",
    "    report = classification_report(authors_test, y_preds)\n",
    "    values = np.array(list(map(float, report.split('\\n')[-2].split()[3:6])))\n",
    "    for i, y_pred in enumerate(y_preds):\n",
    "        y_true = authors_test[i]\n",
    "        print('True author is %s, predict author is %s' % (authors_dict[y_true], authors_dict[y_pred]))\n",
    "    return values\n",
    "        \n",
    "print('Prediction with function words:')\n",
    "FWvals = predictWithExtractor(svmFW, FWextractor)\n",
    "print('\\nPrediction with unigram and bigram:')\n",
    "NGvals = predictWithExtractor(svmNG, NGextractor)\n",
    "print('\\nPrediction with unigram TfIdf:')\n",
    "TfIdfvals = predictWithExtractor(svmTfIdf, TfIdfextractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another model -- based on custom features\n",
    "I found this paper online: http://cs229.stanford.edu/proj2013/StankoLuHsu-AuthorIdentification.pdf which claims to have accuracy comparable to that of standard classifiers but with a significantly shorter runtime. So I tried it. The feature I use in this model is 62 dimension:\n",
    "feature[0]: # of unique words\n",
    "feature[1:27]: distribution for sentence length from length 1 to length 27 or more.\n",
    "feature[27:45]: distribution for word length from length 1 to length 18 or more.\n",
    "feature[45:55]: distribution for pronoun per sentence from length 1 to 10 or more.\n",
    "feature[55:62]: distribution for conjunction per sentence from length 1 to 7 or more.\n",
    "The next two cells define the required utility functions, models and do the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def helper(arr, n):\n",
    "    ret = [0]*n\n",
    "    for i in arr:\n",
    "        if i >= n:\n",
    "            ret[n-1] += 1\n",
    "        elif i > 0:\n",
    "            ret[i-1] += 1\n",
    "    return ret\n",
    "def sentenceLengths(tokenized_sentences):  \n",
    "    t = map(len, tokenized_sentences)\n",
    "    return helper(t, 26)\n",
    "def wordLengths(words):\n",
    "    t = map(len, words)\n",
    "    return helper(t, 18)\n",
    "def pronounPerSentence(tokenized_sentences):\n",
    "    pronouns = set(['something', 'thou', 'everybody', 'all', 'anything', 'your', 'ours', 'her', 'myself', 'him', \n",
    "                    'everything', 'I', 'nobody', 'somebody', 'whomever', 'who', 'everyone', 'none', 'each', 'thee', \n",
    "                    'thy', 'anybody', 'nothing', 'this', 'one', 'our', 'his', 'we', 'yourself', 'they', 'another', \n",
    "                    'himself', 'me', 'several', 'hers', 'no one', 'ourselves', 'both', 'some', 'itself', 'my', \n",
    "                    'whose', 'these', 'other', 'either', 'someone', 'few', 'themselves', 'thine', 'whichever', \n",
    "                    'neither', 'as', 'he', 'she', 'theirs', 'which', 'such', 'mine', 'whom', 'that', 'yourselves', \n",
    "                    'whoever', 'what', 'those', 'others', 'whatever', 'it', 'them', 'us', 'anyone', 'their', 'most', \n",
    "                    'yours', 'any', 'many', 'herself', 'you'])\n",
    "    def countPronouns(sentence):\n",
    "        cnt = 0\n",
    "        for word in sentence:\n",
    "            if word in pronouns:\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "    pronounCnts = map(countPronouns, tokenized_sentences)\n",
    "    return helper(pronounCnts, 10)\n",
    "def conujnctionPerSentence(tokenized_sentences):\n",
    "    conjunctions = set(['and','that','but','or','as','if','when','than','because','while','where',\n",
    "                        'after','so','though','since','until','whether','before','although','nor','like',\n",
    "                        'once','unless','now','except'])\n",
    "    def countConjunctions(sentence):\n",
    "        cnt = 0\n",
    "        for word in sentence:\n",
    "            if word in conjunctions:\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "    conjunctionCnts = map(countConjunctions, tokenized_sentences)\n",
    "    return helper(conjunctionCnts, 7)\n",
    "def customFeatureExtractor(document):\n",
    "    features = []\n",
    "    # number of unique words\n",
    "    word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = word_tokenizer.tokenize(document.lower())\n",
    "    features.append(len(set(words)))\n",
    "    # sentence lengths\n",
    "    sentences = sent_tokenize(document.lower())\n",
    "    tokenized_sentences = list(map(word_tokenizer.tokenize, sentences))\n",
    "    sl = sentenceLengths(tokenized_sentences)\n",
    "    features.extend(sl)\n",
    "    # word lengths\n",
    "    wl = wordLengths(words)\n",
    "    features.extend(wl)\n",
    "    # pronoun per sentences\n",
    "    pps = pronounPerSentence(tokenized_sentences)\n",
    "    features.extend(pps)\n",
    "    # conjunction per sentences\n",
    "    cps = conujnctionPerSentence(tokenized_sentences)\n",
    "    features.extend(cps)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svmCustom():\n",
    "    X = np.asarray(list(map(customFeatureExtractor, documents)))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, authors, test_size=0.2, random_state=2)\n",
    "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "    svm = SVC()\n",
    "    grid = grid_search.GridSearchCV(svm, parameters)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print('Best parameters:', grid.best_params_)\n",
    "    y_true, y_pred = y_val, grid.predict(X_val)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(report)\n",
    "    return grid\n",
    "svmCF = svmCustom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run prediction for our 4th model\n",
    "Based on the training report, it looks that using custom features doesn't give as good performance as the previous 3 models. Similar results occur when running prediction, we see many mis-predicted outcomes. Moreover, the runtime is also not faster much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(list(map(customFeatureExtractor, documents_test)))\n",
    "y_preds = svmCF.predict(X)\n",
    "report = classification_report(authors_test, y_preds)\n",
    "CFvals = np.array(list(map(float, report.split('\\n')[-2].split()[3:6])))\n",
    "for i, y_pred in enumerate(y_preds):\n",
    "    y_true = authors_test[i]\n",
    "    print('True author is %s, predict author is %s' % (authors_dict[y_true], authors_dict[y_pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "Now let's plot the results of 4 models. TfIdf based models performs surprisingly good. Ngrams and function word models are very close but ngrams slightly better. Custom features is the worst among four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_xticks = ['Precision', 'Recall', 'F1-Score']\n",
    "x = np.array([2, 3, 4])\n",
    "plt.xticks(x, my_xticks)\n",
    "plt.xlim(0,4.5)\n",
    "\n",
    "p1 = plt.plot(x, FWvals, '-o')\n",
    "p2 = plt.plot(x, NGvals, '-o')\n",
    "p3 = plt.plot(x, TfIdfvals, '-o')\n",
    "p4 = plt.plot(x, CFvals, '-o')\n",
    "plt.legend((p1[0], p2[0], p3[0], p4[0]), ('Function words', 'Ngrams', 'TfIdf', 'Custom Features'), loc=2)\n",
    "plt.title('Performance for 4 models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional questions:\n",
    "1. How would you assess the performances of your system?  \n",
    "It's a classification problem so I use precision, recall and f1-score.\n",
    "2. Could your system be used to generate novel content such that it appears as being written by a given author?  \n",
    "No. The models I built here only predicts the most likely author. If the task is to generate novel content given an author, then I will use deep learning and language modeling (use LSTM networks to do text generation) instead.\n",
    "3. Is your system scalable w.r.t. number of documents / users? If not, how would address the scalability (in terms of algorithms, infrastructure, or both)?  \n",
    "For this small project, I load everything to memory, so if we have a large number of documents and users that exceeds memory limit, it won't work. For scalability, I think more powerful hardware/infrastructure certainly helps but on the algorithm side, I would try to perform feature extraction one at a time and instead of keeping everything in memory for training, use a data generator that iteratively load extracted feature matrices from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
